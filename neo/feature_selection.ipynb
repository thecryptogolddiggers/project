{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.neural_network import MLPClassifier as NeuralNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef as mcc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "input_file = 'data/neo_price_new.csv'\n",
    "\n",
    "def two_scales(ax1, title, time, data1, data2, c1, c2):\n",
    "    plt.title(str(title))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(time, data1, color=c1)\n",
    "    ax2.plot(time, data2, color=c2)\n",
    "    return ax1, ax2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attribute to predict (binary)\n",
    "attribute = 'increase_flag'\n",
    "\n",
    "# train/test split\n",
    "split = 0.8\n",
    "\n",
    "# number of features available\n",
    "num_feats = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1ad87406ff6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#             min_samples_split=4, min_samples_leaf=2, oob_score=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m175\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# predictions and plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sulkuatam/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \"\"\"\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sulkuatam/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sulkuatam/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 43\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# load and format data\n",
    "df = pd.read_csv(input_file)\n",
    "outcomes = np.array(['no','yes'])\n",
    "names = list(df.columns.values)\n",
    "\n",
    "# preprocessing: factorize date column\n",
    "# for i in range(1):\n",
    "#     df.iloc[:,i] = labelencoder.fit_transform(df.iloc[:,i])\n",
    "    \n",
    "# train/test split\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= split\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "\n",
    "# features (using all)\n",
    "features = df.columns[1:num_feats]\n",
    "\n",
    "# random forest\n",
    "# forest = RFC(n_estimators=175, criterion='gini', max_features='sqrt', max_depth=12,\n",
    "#             min_samples_split=4, min_samples_leaf=2, oob_score=True)\n",
    "forest = RFC(n_estimators=175, criterion='gini', n_jobs=2)\n",
    "forest.fit(train[features], train[attribute])\n",
    "\n",
    "# predictions and plotting\n",
    "preds = outcomes[forest.predict(test[features])]\n",
    "accuracy = accuracy_score(test[attribute], forest.predict(test[features]), \n",
    "                          normalize=True, sample_weight=None) * 100\n",
    "print(\"We accurately predicted the next day's price movement with %.2f%% accuracy.\\n\" % accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(pd.crosstab(index=test[attribute], columns=preds, \n",
    "                  rownames=['actual'], colnames=['preds']))\n",
    "\n",
    "# matthews correction coefficient\n",
    "coeff = mcc(test[attribute],forest.predict(test[features]))\n",
    "print(\"MCC = %.2f\" % coeff)\n",
    "\n",
    "# plot feature importance\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], \n",
    "         color='g', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_l = []\n",
    "\n",
    "for i in range(1,num_feats+1):\n",
    "    # how many?\n",
    "    feats_to_keep = i\n",
    "\n",
    "    # reduce features\n",
    "    top_features = df.columns[indices[-1*int(feats_to_keep):]]\n",
    "\n",
    "    # random forest\n",
    "    forest = RFC(n_jobs=2,n_estimators=100)\n",
    "    forest.fit(train[top_features], train[attribute])\n",
    "    \n",
    "    # predictions and plotting\n",
    "    preds = forest.predict(test[top_features])\n",
    "    mattcc = mcc(test[attribute],preds)\n",
    "    accuracy_l.append(accuracy_score(test[attribute], preds, normalize=True, sample_weight=None) * mattcc * 100)\n",
    "\n",
    "# visual inspection of accuracy W.R.T \n",
    "plt.title('Adjusted Accuracy vs. Features Utilized')\n",
    "plt.bar(range(num_feats), accuracy_l, color='b', align='center')\n",
    "plt.xlabel('n-Top Features Utilized')\n",
    "plt.show()\n",
    "\n",
    "# optimal number of features to keep\n",
    "best_n = accuracy_l.index(max(accuracy_l))\n",
    "print(\"By using %d of %d features, we attained %.2f%% adjusted accuracy\\n\" % (best_n + 1,num_feats,accuracy_l[best_n]))\n",
    "feats_to_keep = best_n\n",
    "\n",
    "# FOR NEXT CELL\n",
    "\n",
    "# reduce features\n",
    "top_features = df.columns[indices[-1*int(feats_to_keep):]]\n",
    "\n",
    "# random forest\n",
    "forest = RFC(n_jobs=4,n_estimators=200)\n",
    "forest.fit(train[top_features], train[attribute])\n",
    "preds = forest.predict(test[top_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attribute to predict (binary)\n",
    "attribute = 'increase_flag'\n",
    "\n",
    "# train/test split\n",
    "split = 0.8\n",
    "\n",
    "# number of features available\n",
    "num_feats = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "df_ada = pd.read_csv(input_file)\n",
    "\n",
    "# preprocessing: factorize date column\n",
    "# for i in [0]:\n",
    "#     df_ada.iloc[:,i] = labelencoder.fit_transform(df_ada.iloc[:,i])\n",
    "\n",
    "# train/test split\n",
    "df_ada['is_train'] = np.random.uniform(0, 1, len(df_ada)) <= split\n",
    "train, test = df_ada[df_ada['is_train']==True], df_ada[df_ada['is_train']==False]\n",
    "\n",
    "# features (using all)\n",
    "features = df_ada.columns[0:num_feats]\n",
    "\n",
    "# training and testing\n",
    "adaboost = ABC(n_estimators=100).fit(train[features], train[attribute])\n",
    "print(\"AdaBoost Accuracy: %.2f%%\" % (adaboost.score(test[features], test[attribute])*100.0))\n",
    "\n",
    "# matthews correction coefficient\n",
    "coeff = mcc(test[attribute],adaboost.predict(test[features]))\n",
    "print(\"MCC = %.2f\" % coeff)\n",
    "\n",
    "# plot feature importance\n",
    "importances = adaboost.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], \n",
    "         color='g', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_l = []\n",
    "\n",
    "for i in range(1,num_feats+1):\n",
    "    # how many?\n",
    "    feats_to_keep = i\n",
    "\n",
    "    # reduce features\n",
    "    top_features = df_ada.columns[indices[-1*int(feats_to_keep):]]\n",
    "\n",
    "    # adaboost\n",
    "    adaboost = ABC(n_estimators=100).fit(train[features], train[attribute])\n",
    "    adaboost.fit(train[top_features], train[attribute])\n",
    "    \n",
    "    # predictions and plotting\n",
    "    coeff = mcc(test[attribute],adaboost.predict(test[top_features]))\n",
    "    accuracy_l.append(adaboost.score(test[top_features], test[attribute]) * coeff * 100.0)\n",
    "\n",
    "plt.title('Adjusted Accuracy vs. Features Utilized')\n",
    "plt.bar(range(num_feats), accuracy_l, color='b', align='center')\n",
    "plt.xlabel('n-Top Features Utilized')\n",
    "plt.show()\n",
    "\n",
    "best_n = accuracy_l.index(max(accuracy_l))\n",
    "\n",
    "print(\"By using %d of %d features, we attained %.2f%% adjusted accuracy\\n\" % (best_n + 1,num_feats,accuracy_l[best_n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attribute to predict (binary)\n",
    "attribute = 'increase_flag'\n",
    "\n",
    "# train/test split\n",
    "split = 0.8\n",
    "\n",
    "# number of features available\n",
    "num_feats = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "df_gb = pd.read_csv(input_file)\n",
    "\n",
    "# preprocessing: factorize date column\n",
    "# for i in [0]:\n",
    "#     df_gb.iloc[:,i] = labelencoder.fit_transform(df_gb.iloc[:,i])\n",
    "\n",
    "# train/test split\n",
    "df_gb['is_train'] = np.random.uniform(0, 1, len(df_gb)) <= split\n",
    "train, test = df_gb[df_gb['is_train']==True], df_gb[df_gb['is_train']==False]\n",
    "\n",
    "# features (using all)\n",
    "features = df_gb.columns[0:num_feats]\n",
    "\n",
    "# training and testing\n",
    "gradboost = GBC(n_estimators=100, learning_rate=0.01, max_depth=10, random_state=5)\n",
    "gradboost.fit(train[features], train[attribute])\n",
    "print(\"GradBoost Accuracy: %.2f%%\" % (gradboost.score(test[features], test[attribute])*100.0))\n",
    "\n",
    "# matthews correction coefficient\n",
    "coeff = mcc(test[attribute],gradboost.predict(test[features]))\n",
    "print(\"MCC = %.2f\" % coeff)\n",
    "\n",
    "# feature importance\n",
    "importances = gradboost.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], \n",
    "         color='g', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_l = []\n",
    "for i in range(1,num_feats+1):\n",
    "    # how many?\n",
    "    feats_to_keep = i\n",
    "    # reduce features\n",
    "    top_features = df_gb.columns[indices[-1*int(feats_to_keep):]]\n",
    "    # gradient boost\n",
    "    gradboost = GBC(n_estimators=100, learning_rate=0.01, max_depth=10, random_state=5)\n",
    "    gradboost.fit(train[top_features], train[attribute])    \n",
    "    # predictions and plotting\n",
    "    coeff = mcc(test[attribute],gradboost.predict(test[top_features]))\n",
    "    accuracy_l.append(gradboost.score(test[top_features], test[attribute]) * coeff * 100.0)\n",
    "\n",
    "# plot feature selection results\n",
    "plt.title('Prediction Accuracy vs. Features Utilized')\n",
    "plt.bar(range(num_feats), accuracy_l, color='b', align='center')\n",
    "plt.xlabel('n-Top Features Utilized')\n",
    "plt.show()\n",
    "\n",
    "# optimal number of features\n",
    "best_n = accuracy_l.index(max(accuracy_l))\n",
    "print(\"By using %d of %d features, we attained %.2f%% accuracy\\n\" % (best_n + 1,num_feats,accuracy_l[best_n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attribute to predict (binary)\n",
    "attribute = 'increase_flag'\n",
    "\n",
    "# train/test split\n",
    "split = 0.8\n",
    "\n",
    "# number of features available\n",
    "num_feats = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "df_nn = pd.read_csv(input_file)\n",
    "\n",
    "# preprocessing: factorize date column\n",
    "# for i in [0]:\n",
    "#     df_nn.iloc[:,i] = labelencoder.fit_transform(df_nn.iloc[:,i])\n",
    "\n",
    "# train/test split\n",
    "df_nn['is_train'] = np.random.uniform(0, 1, len(df_nn)) <= split\n",
    "train, test = df_nn[df_nn['is_train']==True], df_nn[df_nn['is_train']==False]\n",
    "\n",
    "# features (using all)\n",
    "features = df_nn.columns[0:num_feats]\n",
    "\n",
    "# transformations\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(train[features])  \n",
    "train[features] = scaler.transform(train[features])  \n",
    "test[features] = scaler.transform(test[features])  \n",
    "\n",
    "# predict whether price will drop more than 5% tomorrow\n",
    "nn = NeuralNet(solver='lbfgs', learning_rate_init=1e-3, alpha=1e-5, hidden_layer_sizes=(100,5), random_state=1, verbose=False)\n",
    "nn.fit(train[features], train[attribute])\n",
    "print(\"Neuralnet Accuracy: %.2f%%\" % (nn.score(test[features], test[attribute])*100.0))\n",
    "\n",
    "# matthews correction coefficient\n",
    "coeff = mcc(test[attribute],nn.predict(test[features]))\n",
    "print(\"MCC = %.2f\" % coeff)\n",
    "\n",
    "\n",
    "# # predict whether price will drop more than 5% tomorrow\n",
    "# nn.fit(train[features], train[attribute])\n",
    "# large_price_drop_prediction_5 = nn.predict(test[features])\n",
    "# probs_5_drop = nn.predict_proba(test[features])\n",
    "# print(nn.score(test[features], test[attribute]))\n",
    "\n",
    "# # predict whether price will rise more than 5% tomorrow\n",
    "# nn.fit(train[features], train[attribute])\n",
    "# large_price_gain_prediction_5 = nn.predict(test[features])\n",
    "# probs_5_gain = nn.predict_proba(test[features])\n",
    "# print(nn.score(test[features], test[attribute]))\n",
    "\n",
    "# predict whether price will increase or decrease\n",
    "# nn.fit(train[features], train[attribute])\n",
    "# price_increases_tomorrow = nn.predict(test[features])\n",
    "# probs_inc = nn.predict_proba(test[features])\n",
    "# print(nn.score(test[features], test[attribute]))\n",
    "\n",
    "# # predict whether price will rise more than 5% tomorrow\n",
    "# nn.fit(train[features], train['2_gain_tomorrow'])\n",
    "# large_price_gain_prediction_2 = nn.predict(test[features])\n",
    "# probs_2_gain = nn.predict_proba(test[features])\n",
    "# print(nn.score(test[features], test['2_gain_tomorrow']))\n",
    "\n",
    "# nn.fit(train[features], train['2_drop_tomorrow'])\n",
    "# large_price_drop_prediction_2 = nn.predict(test[features])\n",
    "# probs_2_drop = nn.predict_proba(test[features])\n",
    "# print(nn.score(test[features], test['2_drop_tomorrow']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_l = []\n",
    "for i in range(1,num_feats+1):\n",
    "    # how many?\n",
    "    feats_to_keep = i\n",
    "    # reduce features\n",
    "    top_features = df_nn.columns[indices[-1*int(feats_to_keep):]]\n",
    "    # gradient boost\n",
    "    nn = NeuralNet()\n",
    "    nn.fit(train[top_features], train[attribute])    \n",
    "    # predictions and plotting\n",
    "    coeff = mcc(test[attribute],nn.predict(test[top_features]))\n",
    "    accuracy_l.append(nn.score(test[top_features], test[attribute]) * coeff * 100.0)\n",
    "\n",
    "# plot feature selection results\n",
    "plt.title('Prediction Accuracy vs. Features Utilized')\n",
    "plt.bar(range(num_feats), accuracy_l, color='b', align='center')\n",
    "plt.xlabel('n-Top Features Utilized')\n",
    "plt.show()\n",
    "\n",
    "# optimal number of features\n",
    "best_n = accuracy_l.index(max(accuracy_l))\n",
    "print(\"By using %d of %d features, we attained %.2f%% accuracy\\n\" % (best_n + 1,num_feats,accuracy_l[best_n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
